{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter8 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 어텐션 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 seq2seq의 문제점\n",
    "- Encoder의 출력은 '고정 길이의 벡터'인데, 고정 길이 벡터는 입력 문장의 길이에 관계없이 항상 같은 길이의 벡터로 변환\n",
    "- 이렇게 되면 필요한 정보가 벡터에 다 담기지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Encoder 개선\n",
    "- Encoder 출력의 길이를 입력 문장의 길이에 따라 바꿔주기\n",
    "- Encoder가 출력하는 hs 행렬은 각 단어에 해당하는 벡터들의 집합\n",
    "- keras RNN 계층의 초기화 인수 return_sequences=True : 모든 시각의 은닉상태벡터를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 Decoder 개선1\n",
    "- '입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가'라는 대응 관계\n",
    "- 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표 = 어텐션\n",
    "- 각 단어의 중요도(기여도)를 나타내는 '가중치'를 별도로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar.shape: (5, 4)\n",
      "[[0.8  0.8  0.8  0.8 ]\n",
      " [0.1  0.1  0.1  0.1 ]\n",
      " [0.03 0.03 0.03 0.03]\n",
      " [0.05 0.05 0.05 0.05]\n",
      " [0.02 0.02 0.02 0.02]]\n",
      "t.shape: (5, 4)\n",
      "[[ 0.92213466  0.07993642  1.29287305  0.07689452]\n",
      " [-0.10521151  0.20460277  0.02612539 -0.12710195]\n",
      " [-0.01691926  0.04252966  0.03065717  0.0320055 ]\n",
      " [-0.03366714 -0.11450667  0.0043201   0.02875747]\n",
      " [-0.00352427 -0.0051169   0.01249374 -0.00423493]]\n",
      "c.shape: (4,)\n",
      "[0.76281247 0.20744527 1.36646946 0.00632062]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(\"ar.shape:\", ar.shape)\n",
    "print(ar)\n",
    "\n",
    "t = hs * ar\n",
    "print(\"t.shape:\", t.shape)\n",
    "print(t)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(\"c.shape:\", c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.shape: (10, 5, 4)\n",
      "[[[-1.04365301e-01  3.46721366e-01 -1.25917344e+00  6.32272341e-01]\n",
      "  [ 1.16635638e-01  1.62190188e-01 -3.31974348e-02  3.21264407e-01]\n",
      "  [-7.85581009e-02  1.86633058e-02 -3.16135788e-02  2.59185062e-03]\n",
      "  [-1.28428374e+00 -5.68648650e-02 -3.52974851e-02  3.61573273e+00]\n",
      "  [ 4.57126053e-01  3.22427408e-01 -1.08428927e-01  4.66074483e-01]]\n",
      "\n",
      " [[ 1.42994299e-02 -2.48865276e-02  6.17457122e-02  4.90816363e-03]\n",
      "  [-7.01841558e-02  3.40481054e-02 -8.59803326e-03  2.70145386e-02]\n",
      "  [ 1.58392343e-02  4.04597238e-02 -8.56287271e-03 -2.88751200e-02]\n",
      "  [ 7.26383568e-02  3.60468792e-01 -2.62714540e-01 -3.61423267e-01]\n",
      "  [ 2.24731729e+00 -1.40432568e+00  2.02458719e-01  8.68408745e-01]]\n",
      "\n",
      " [[ 6.60921302e-01 -8.85002893e-02 -2.10294629e-01 -8.18820605e-01]\n",
      "  [-3.91682368e-01 -6.26245052e-01 -1.03940004e-01 -4.45049267e-01]\n",
      "  [ 1.90644731e-01 -2.96266806e-01  8.85724501e-01  8.90593485e-01]\n",
      "  [-1.64211029e+00 -4.42388044e-01  3.23856967e+00  4.42868455e-01]\n",
      "  [ 5.55492797e-01  3.59211782e-01 -1.94130394e+00 -2.49954325e-01]]\n",
      "\n",
      " [[-2.37099264e-02  4.76032205e-01 -4.26873609e-01 -3.85851002e-01]\n",
      "  [ 1.36321755e-01  6.01882440e-01  4.75884627e-01 -4.75503168e-01]\n",
      "  [ 4.80175586e-01  8.06662206e-02  1.06837829e+00 -6.69381284e-01]\n",
      "  [-1.50888526e+00  1.09635604e+00  1.74656132e+00  2.18199747e+00]\n",
      "  [-1.21381191e-01  8.21889261e-02 -7.26581119e-02 -3.12311377e-02]]\n",
      "\n",
      " [[ 1.45607622e-01 -2.68348428e-01 -6.43879571e-01 -1.29974240e-01]\n",
      "  [-4.75157624e-01 -4.82987228e-01  1.86992075e-01 -4.82752660e-01]\n",
      "  [ 1.60613706e+00 -6.30389289e-01 -4.09009444e-01 -2.03968608e+00]\n",
      "  [-3.58682444e-01 -1.01395843e+00  3.65799761e-01  6.66166539e-01]\n",
      "  [ 8.25222159e-02 -6.90020292e-01  2.06869722e-02  1.64169662e-01]]\n",
      "\n",
      " [[-1.34102764e-01 -6.55558307e-01  1.09724362e+00 -3.95033311e-01]\n",
      "  [ 6.21330493e-01  3.93521734e-01 -4.77164728e-01  3.60859277e-03]\n",
      "  [ 7.80228475e-02  2.99368308e-01  3.11337888e-01  5.33785977e-01]\n",
      "  [-5.00249070e-01  4.75421613e-01  4.09795449e-01  4.60753825e-01]\n",
      "  [ 2.08788581e-01  8.03128001e-01 -9.44315090e-01 -6.75763390e-01]]\n",
      "\n",
      " [[-1.78449090e-01 -2.55005145e-01 -1.13540428e-01 -1.81841781e-02]\n",
      "  [ 2.18847227e-01  2.11151221e-01 -7.92966333e-02  2.45135532e-01]\n",
      "  [-2.06901935e-01 -2.56724971e-01 -7.86649911e-02 -1.14936616e-01]\n",
      "  [ 1.40105274e+00  1.24555685e+00 -7.03172014e-01  6.70427957e-01]\n",
      "  [-1.66265461e+00  6.00368323e-01 -2.29543471e-01  1.71975247e+00]]\n",
      "\n",
      " [[ 4.14265282e-01  1.26964057e+00 -4.32386771e-01 -4.51088539e-01]\n",
      "  [-1.02109477e+00  1.52800661e+00  1.23083199e+00 -5.48955067e-01]\n",
      "  [ 3.74729782e-01  3.68259745e-01  1.34212181e+00 -2.39988739e-01]\n",
      "  [ 3.28226403e-02 -1.03325384e+00 -1.82851847e-02 -4.68273916e-01]\n",
      "  [ 1.97476709e+00  3.23739160e-01 -1.61660327e+00 -1.52277726e+00]]\n",
      "\n",
      " [[ 5.22500439e-02 -3.71925718e-02  3.26306384e-01 -5.72361710e-01]\n",
      "  [ 1.38482213e-01  1.20483285e-01  1.34191939e-02  1.39754220e-01]\n",
      "  [ 2.13939711e-01  9.79409875e-02 -3.63704833e-01 -4.39719084e-01]\n",
      "  [-4.91658714e-01  4.63619223e-01 -1.04820267e+00  7.32952968e-01]\n",
      "  [ 2.97969224e-01 -2.84020762e-02 -5.93620431e-02 -1.07295702e-02]]\n",
      "\n",
      " [[ 5.92232385e-01 -3.17611432e-01  1.33903782e+00  2.24966894e-01]\n",
      "  [-5.49281352e-01  3.62237261e-01  9.90318124e-01 -1.00077666e+00]\n",
      "  [ 6.14734346e-02  7.11317708e-01  8.03052573e-02 -6.72203605e-01]\n",
      "  [-3.75415451e-01  5.20444903e-01 -1.24998247e-01  2.89804365e-01]\n",
      "  [-7.94625276e-01  8.52273129e-01 -3.11451875e-01 -7.82393169e-01]]]\n",
      "c.shape: (10, 4)\n",
      "[[-0.89344545  0.7931374  -1.46771087  5.03793581]\n",
      " [ 2.27991016 -0.99423559 -0.01567102  0.51003306]\n",
      " [-0.62673382 -1.09418841  1.86875559 -0.18036226]\n",
      " [-1.03747904  2.33712584  2.79129253  0.62003088]\n",
      " [ 1.00042683 -3.08570366 -0.47941021 -1.82207678]\n",
      " [ 0.27379009  1.31588135  0.39689714 -0.07264831]\n",
      " [-0.42810567  1.54534628 -1.20421754  2.50219517]\n",
      " [ 1.77549002  2.45639225  0.50567857 -3.23108352]\n",
      " [ 0.21098248  0.61644885 -1.13154397 -0.15010318]\n",
      " [-1.06561626  2.12866157  1.97321108 -1.94060217]]\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "# ar = a.reshape(N, T, 1)   브로드캐스트사용\n",
    "\n",
    "t = hs * ar\n",
    "print(\"t.shape:\", t.shape)\n",
    "print(t)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(\"c.shape:\", c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs*ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)   # sum의 역전파\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)   # repeat의 역전파\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4 Decoder 개선2\n",
    "- Decoder의 LSTM 계층의 은닉 상태 벡터를 h라고 하는데, h가 hs의 각 단어 벡터와 얼마나 비슷한가를 수치로 나타내기\n",
    "- 내적 사용 = 두 벡터가 얼마나 같은 방향을 향하고 있는가 (유사도)\n",
    "- h와 hs이 각 단어 벡터와의 유사도 구한 후 s(score)를 구하고 softmax 함수 적용(정규화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.shape: (10, 5, 4)\n",
      "s.shape: (10, 4)\n",
      "a.shape: (10, 4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "# ar = a.reshape(N, T, 1)   브로드캐스트사용\n",
    "\n",
    "t = hs * ar\n",
    "print(\"t.shape:\", t.shape)\n",
    "\n",
    "s = np.sum(t, axis=1)\n",
    "print(\"s.shape:\", s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(\"a.shape:\", a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)     #.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5 Decoder 개선3\n",
    "- Attention Weight과 Weight Sum 결합\n",
    "- Attention Weight 계층: Encoder가 출력하는 각 단어의 벡터 hs에 주목하여 해당 단어의 가중치 a를 구함\n",
    "- Weight Sum 계층: a와 hs의 가중합을 구하고 그 결과를 맥락 벡터 c로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.seq2seq import Encoder, Seq2seq\n",
    "from common.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.seq2seq import Encoder, Seq2seq\n",
    "from common.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.seq2seq import Encoder, Seq2seq\n",
    "from common.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 날짜 형식 변환 문제\n",
    "- dataset/date.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 1[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 20[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 43[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 64[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 84[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 104[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 125[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 145[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 165[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 186[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 207[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 228[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 249[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 269[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 290[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 310[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 330[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 349[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 1[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 21[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 41[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 61[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 82[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 102[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 123[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 143[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 163[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 184[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 206[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 226[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 246[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 266[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 287[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 308[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 328[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 348[s] | loss 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "정확도 51.680%\n",
      "| epoch 3 |  iter 1 / 351 | time 1[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 22[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 42[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 63[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 87[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 108[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 129[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 149[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 170[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 191[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 212[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 232[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 252[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 272[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 292[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 313[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 334[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 354[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 1[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 23[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 44[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 65[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 86[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 107[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 128[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 149[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 169[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 190[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 211[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 231[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 251[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 273[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 293[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 315[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 335[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 356[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| epoch 5 |  iter 1 / 351 | time 1[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 21[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 145[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 166[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 187[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 208[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 229[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 249[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 270[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 291[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 311[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 332[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 353[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| epoch 6 |  iter 1 / 351 | time 1[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 22[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 43[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 85[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 126[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 147[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 168[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 188[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 209[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 230[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 251[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 272[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 292[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 313[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 334[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 355[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1994-05-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 2013-08-22\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-01-06\n",
      "---\n",
      "정확도 81.040%\n",
      "| epoch 7 |  iter 1 / 351 | time 1[s] | loss 0.11\n",
      "| epoch 7 |  iter 21 / 351 | time 23[s] | loss 0.03\n",
      "| epoch 7 |  iter 41 / 351 | time 43[s] | loss 0.01\n",
      "| epoch 7 |  iter 61 / 351 | time 63[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 84[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 126[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 147[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 169[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 189[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 210[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 231[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 251[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 272[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 292[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 312[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 333[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 354[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| epoch 8 |  iter 1 / 351 | time 1[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 22[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 43[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 84[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 127[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 148[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 169[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 190[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 210[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 231[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 251[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 273[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 294[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 314[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 336[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 356[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| epoch 9 |  iter 1 / 351 | time 2[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 22[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 43[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 85[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 107[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 128[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 149[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 171[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 192[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 213[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 233[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 254[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 275[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 296[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 317[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 337[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 358[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| epoch 10 |  iter 1 / 351 | time 1[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 21[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 104[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 125[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 146[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 166[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 187[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 207[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 228[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 248[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 270[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 291[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 312[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 332[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 353[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from common.attention_seq2seq import AttentionSeq2seq\n",
    "from common.seq2seq import Seq2seq\n",
    "from common.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 어텐션 시각화\n",
    "- common.visualize_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 어텐션에 관한 남은 이야기\n",
    "\n",
    "### 8.4.1 양방향 RNN\n",
    "- LSTM 계층에 더해 역방향으로 처리하는 LSTM 게층 추가\n",
    "- 각 시각에서는 이 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉 상태로 처리\n",
    "- 양방향으로 처리함으로써 각 단어에 대응하는 은닉 상태 벡터에는 좌와 우 양쪽 방향으로부터 정보 집약\n",
    "- 따라서 균형 잡힌 정보 인코딩\n",
    "\n",
    "### 8.4.2 Attention 계층 사용 방법\n",
    "\n",
    "### 8.4.3 seq2seq 심층화와 skip 연결\n",
    "- RNN 계층을 깊게 쌓기, 층을 깊게 쌓으면 표현력 높은 모델을 만들 수 있다\n",
    "- skip 연결 : 계층을 넘어 선을 연결\n",
    "- 따라서 층이 깊어져도 기울기 소실, 폭발되지 않고 전파됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 어텐션 응용\n",
    "\n",
    "### 8.5.1 구글 신경망 기계 번역 (GNMT)\n",
    "- 신경망 기계 번역 Neural Machine Translation\n",
    "\n",
    "### 8.5.2 트랜스포머\n",
    "- 'RNN이 아닌' 어텐션 사용\n",
    "- 셀프어텐션 self-attention : 하나의 시계열 데이터 내에서 각 원소가 다른 원소들과 어떻게 관련되는지 살펴보기\n",
    "\n",
    "### 8.5.3 뉴럴 튜링 머신 (NTM)\n",
    "- 외부 메모리를 통한 확장\n",
    "- Encoder와 Decoder는 '메모리 조작' 같은 작업을 수행\n",
    "- RNN의 외부에 정보 저장용 메모리 기능을 배치하고 어텐션을 이용하여 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
